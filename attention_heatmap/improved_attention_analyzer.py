#!/usr/bin/env python3
"""
Improved Attention Analyzer
åŸºäºåˆ˜ä¸–è¶…çš„å®éªŒå‘ç°æ”¹è¿›çš„attentionåˆ†æå™¨

æ ¸å¿ƒæ”¹è¿›ï¼š
1. ä½¿ç”¨æ›´å¼ºçš„Qwen3-4Bæ¨¡å‹
2. åˆ†æç¬¬ä¸€å±‚attentionï¼ˆæœ€æ¥è¿‘åŸå§‹tokenè¯­ä¹‰ï¼‰
3. æ’é™¤é¦–ä¸ªtokençš„æƒé‡å¹²æ‰°
4. å®ç°"æ¯å¸§top3"çš„å¯è¯»åŒ–è¾“å‡º
5. æ”¯æŒå¯é¢„æœŸçš„æ•…äº‹æ¥ç»­ä»»åŠ¡
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import json

class ImprovedAttentionAnalyzer:
    """æ”¹è¿›çš„attentionåˆ†æå™¨"""
    
    def __init__(self, model_path: str = r"W:\LLM\Qwen3-4B"):
        print(f"ğŸš€ Loading improved model: {model_path}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            attn_implementation="eager"
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"âœ… Improved analyzer ready with {model_path}")
    
    def extract_clean_generation_attention(self, context: str, max_new_tokens: int = 15) -> Dict:
        """æå–å¹²å‡€çš„generation attentionï¼Œæ’é™¤é¦–ä¸ªtokenå¹²æ‰°"""
        print(f"ğŸ” Extracting clean attention for: {context[:50]}...")
        
        inputs = self.tokenizer(context, return_tensors="pt", padding=False).to(self.device)
        context_tokens = self.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
        context_length = len(context_tokens)
        
        print(f"ğŸ“ Context tokens: {context_tokens}")
        print(f"ğŸ“ Context length: {context_length}")
        
        generation_data = {
            "context": context,
            "context_tokens": context_tokens,
            "context_length": context_length,
            "generated_tokens": [],
            "generation_steps": []
        }
        
        input_ids = inputs.input_ids
        
        for step in range(max_new_tokens):
            with torch.no_grad():
                outputs = self.model(input_ids, output_attentions=True, use_cache=False)
            
            # è·å–ä¸‹ä¸€ä¸ªtoken
            next_token_logits = outputs.logits[0, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)
            next_token = self.tokenizer.convert_ids_to_tokens([next_token_id.item()])[0]
            
            # æå–ç¬¬ä¸€å±‚attentionï¼ˆæœ€æ¥è¿‘åŸå§‹è¯­ä¹‰ï¼‰
            first_layer_att = outputs.attentions[0][0]  # (num_heads, seq_len, seq_len)
            
            # æœ€åä¸€ä¸ªtokenï¼ˆå½“å‰ç”Ÿæˆçš„ï¼‰å¯¹contextçš„attention
            last_token_att = first_layer_att[:, -1, :context_length]  # (num_heads, context_length)
            
            # å¯¹æ‰€æœ‰headæ±‚å¹³å‡
            avg_attention = torch.mean(last_token_att, dim=0).cpu().numpy()  # (context_length,)
            
            # æ’é™¤é¦–ä¸ªtokenï¼Œé‡æ–°å½’ä¸€åŒ–
            if context_length > 1:
                clean_attention = avg_attention[1:]  # æ’é™¤é¦–ä¸ªtoken
                clean_attention = clean_attention / np.sum(clean_attention)  # é‡æ–°å½’ä¸€åŒ–
                clean_context_tokens = context_tokens[1:]  # å¯¹åº”çš„tokenåˆ—è¡¨
            else:
                clean_attention = avg_attention
                clean_context_tokens = context_tokens
            
            # è·å–top3ç›¸å…³token
            top3_indices = np.argsort(clean_attention)[-3:][::-1]  # é™åº
            top3_tokens = [(clean_context_tokens[i], clean_attention[i], i+1)
                          for i in top3_indices if i < len(clean_context_tokens)]

            # è§£ç tokenä¸ºå¯è¯»æ–‡æœ¬
            readable_generated_token = self.tokenizer.decode([next_token_id.item()], skip_special_tokens=True)
            readable_top3 = []
            for token, weight, pos in top3_tokens:
                # å°è¯•è§£ç å•ä¸ªtoken
                try:
                    readable_token = self.tokenizer.decode(self.tokenizer.convert_tokens_to_ids([token]), skip_special_tokens=True)
                    if not readable_token.strip():  # å¦‚æœè§£ç ä¸ºç©ºï¼Œä¿æŒåŸtoken
                        readable_token = token
                except:
                    readable_token = token
                readable_top3.append((readable_token, weight, pos))

            step_data = {
                "step": step,
                "generated_token": next_token,
                "readable_generated_token": readable_generated_token,
                "raw_attention": avg_attention.tolist(),
                "clean_attention": clean_attention.tolist(),
                "top3_related_tokens": top3_tokens,
                "readable_top3_tokens": readable_top3
            }

            generation_data["generation_steps"].append(step_data)
            generation_data["generated_tokens"].append(next_token)

            # è¾“å‡ºå½“å‰æ­¥éª¤çš„top3 - ä½¿ç”¨å¯è¯»ç‰ˆæœ¬
            print(f"Step {step+1}: '{readable_generated_token}' -> Top3: {[(t[0], f'{t[1]:.3f}') for t in readable_top3]}")
            
            # æ›´æ–°input_ids
            input_ids = torch.cat([input_ids, next_token_id], dim=1)
            
            if next_token_id.item() == self.tokenizer.eos_token_id:
                break
        
        return generation_data
    
    def create_story_continuation_test(self) -> str:
        """åˆ›å»ºå°çº¢å¸½æ•…äº‹æ¥ç»­æµ‹è¯•ç”¨ä¾‹"""
        story_context = """æˆ‘æ˜¯å°çº¢å¸½ï¼Œä»Šå¤©è¦å»çœ‹æœ›ç”Ÿç—…çš„å¥¶å¥¶ã€‚æˆ‘æ­£åœ¨æ£®æ—é‡Œçš„å°å±‹ä¸­å‡†å¤‡ç¤¼ç‰©ã€‚çªç„¶ï¼Œæˆ‘å¬åˆ°äº†æ•²é—¨å£°ã€‚æˆ‘å¿ƒæƒ³è¿™ä¹ˆæ™šäº†ä¼šæ˜¯è°å‘¢ï¼Ÿæˆ‘"""
        return story_context
    
    def create_technical_context_test(self) -> str:
        """åˆ›å»ºæŠ€æœ¯ä¸Šä¸‹æ–‡æµ‹è¯•ç”¨ä¾‹"""
        tech_context = """MemoTreeç³»ç»Ÿä½¿ç”¨åˆ†å±‚LODèŠ‚ç‚¹ç®¡ç†è®¤çŸ¥ä¸Šä¸‹æ–‡ã€‚æ¯ä¸ªèŠ‚ç‚¹åŒ…å«Titleã€Briefã€Detailã€Fullå››ä¸ªè¯¦ç»†åº¦çº§åˆ«ã€‚ç³»ç»Ÿæ ¹æ®attentionæƒé‡åŠ¨æ€è°ƒæ•´èŠ‚ç‚¹çš„å±•å¼€çŠ¶æ€ã€‚å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡æ˜¯"""
        return tech_context
    
    def visualize_clean_attention_flow(self, generation_data: Dict):
        """å¯è§†åŒ–æ¸…ç†åçš„attentionæµ"""
        steps = generation_data["generation_steps"]
        context_tokens = generation_data["context_tokens"][1:]  # æ’é™¤é¦–ä¸ªtoken
        generated_tokens = generation_data["generated_tokens"]
        
        if not steps:
            return
        
        # æ„å»ºattentionçŸ©é˜µ
        num_steps = len(steps)
        num_context_tokens = len(context_tokens)
        
        attention_matrix = np.zeros((num_steps, num_context_tokens))
        
        for step_idx, step_data in enumerate(steps):
            clean_attention = step_data["clean_attention"]
            attention_matrix[step_idx, :len(clean_attention)] = clean_attention
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(16, 10))
        
        # ä½¿ç”¨å¯¹æ•°å°ºåº¦å¢å¼ºå¯è§†åŒ–
        log_attention = np.log1p(attention_matrix)
        
        im = plt.imshow(log_attention.T, aspect='auto', cmap='plasma', interpolation='nearest')
        
        plt.title('Clean Generation Attention Flow (First Layer, Excluding First Token)\n'
                 'Y-axis: Context Tokens, X-axis: Generated Tokens', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('Generation Steps', fontsize=12)
        plt.ylabel('Context Tokens (Excluding First)', fontsize=12)
        
        # è®¾ç½®xè½´æ ‡ç­¾
        x_labels = [f"Step{i+1}\n{token[:6]}" for i, token in enumerate(generated_tokens)]
        plt.xticks(range(len(x_labels)), x_labels, rotation=45, ha='right')
        
        # è®¾ç½®yè½´æ ‡ç­¾
        y_step = max(1, num_context_tokens // 15)
        y_indices = range(0, num_context_tokens, y_step)
        y_labels = [context_tokens[i][:8] for i in y_indices if i < len(context_tokens)]
        plt.yticks(y_indices, y_labels)
        
        # é¢œè‰²æ¡
        cbar = plt.colorbar(im)
        cbar.set_label('Log(1 + Clean Attention)', rotation=270, labelpad=20)
        
        plt.tight_layout()
        plt.savefig('clean_attention_flow.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š Clean attention flow saved as clean_attention_flow.png")
    
    def generate_readable_attention_report(self, generation_data: Dict) -> str:
        """ç”Ÿæˆå¯è¯»çš„attentionæŠ¥å‘Š"""
        report_lines = []
        report_lines.append("ğŸ¯ Generation Attention Analysis Report")
        report_lines.append("=" * 50)
        report_lines.append(f"Context: {generation_data['context']}")
        report_lines.append(f"Generated: {' '.join(generation_data['generated_tokens'])}")
        report_lines.append("")
        
        report_lines.append("ğŸ“Š Step-by-Step Attention Analysis:")
        report_lines.append("-" * 40)

        for step_data in generation_data["generation_steps"]:
            step = step_data["step"]
            readable_token = step_data.get("readable_generated_token", step_data["generated_token"])
            readable_top3 = step_data.get("readable_top3_tokens", step_data["top3_related_tokens"])

            report_lines.append(f"Step {step+1}: Generated '{readable_token}'")
            report_lines.append("  Most related context tokens:")
            for i, (related_token, weight, pos) in enumerate(readable_top3):
                report_lines.append(f"    {i+1}. '{related_token}' (pos {pos}): {weight:.4f}")
            report_lines.append("")
        
        report_text = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        with open("attention_analysis_report.txt", "w", encoding="utf-8") as f:
            f.write(report_text)
        
        print("ğŸ“‹ Readable report saved as attention_analysis_report.txt")
        return report_text
    
    def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸ§ª Starting Comprehensive Attention Analysis")
        print("=" * 60)
        
        # æµ‹è¯•1: å°çº¢å¸½æ•…äº‹
        print("\nğŸ“š Test 1: Little Red Riding Hood Story")
        story_context = self.create_story_continuation_test()
        story_data = self.extract_clean_generation_attention(story_context, max_new_tokens=12)
        
        print("\nğŸ“Š Visualizing story attention...")
        self.visualize_clean_attention_flow(story_data)
        
        print("\nğŸ“‹ Generating story report...")
        story_report = self.generate_readable_attention_report(story_data)
        
        # æµ‹è¯•2: æŠ€æœ¯ä¸Šä¸‹æ–‡
        print("\nğŸ”§ Test 2: Technical Context")
        tech_context = self.create_technical_context_test()
        tech_data = self.extract_clean_generation_attention(tech_context, max_new_tokens=10)
        
        print("\nğŸ“Š Visualizing technical attention...")
        self.visualize_clean_attention_flow(tech_data)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results = {
            "story_test": story_data,
            "technical_test": tech_data,
            "model_info": {
                "model_path": "W:/LLM/Qwen3-4B",
                "analysis_layer": "first_layer",
                "excludes_first_token": True
            }
        }
        
        with open("comprehensive_attention_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ Comprehensive results saved!")
        return results

def main():
    analyzer = ImprovedAttentionAnalyzer()
    results = analyzer.run_comprehensive_test()
    return results

if __name__ == "__main__":
    results = main()
