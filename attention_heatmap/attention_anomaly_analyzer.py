#!/usr/bin/env python3
"""
Attention Anomaly Analyzer
æ·±åº¦åˆ†æattentionæƒé‡çš„å¼‚å¸¸æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯<|begin_of_text|>çš„é«˜æƒé‡é—®é¢˜

æ ¸å¿ƒé—®é¢˜ï¼š
1. <|begin_of_text|>ä¸ºä»€ä¹ˆæœ‰å¦‚æ­¤é«˜çš„attentionæƒé‡ï¼Ÿ
2. è¿™æ˜¯æ¨¡å‹çš„è®¾è®¡ç‰¹æ€§è¿˜æ˜¯è®¡ç®—é”™è¯¯ï¼Ÿ
3. å¦‚ä½•æ­£ç¡®è§£é‡Šå’Œå¤„ç†è¿™ç§ç°è±¡ï¼Ÿ
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import json

class AttentionAnomalyAnalyzer:
    """ä¸“é—¨åˆ†æattentionæƒé‡å¼‚å¸¸çš„å·¥å…·"""
    
    def __init__(self, model_path: str = r"W:\LLM\Llama-3.2-3B-Instruct"):
        print(f"ğŸ” Loading model for anomaly analysis: {model_path}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            attn_implementation="eager"
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"âœ… Model loaded for anomaly analysis")
    
    def analyze_bos_token_attention(self, context: str, max_new_tokens: int = 10) -> Dict:
        """ä¸“é—¨åˆ†æ<|begin_of_text|>tokençš„attentionæ¨¡å¼"""
        print(f"ğŸ” Analyzing BOS token attention for: {context[:50]}...")
        
        # Tokenize
        inputs = self.tokenizer(context, return_tensors="pt", padding=False).to(self.device)
        context_tokens = self.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
        
        print(f"ğŸ“ Context tokens: {context_tokens[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ª
        
        bos_analysis = {
            "context": context,
            "context_tokens": context_tokens,
            "bos_token_index": 0 if context_tokens[0].startswith('<|begin_of_text|>') else -1,
            "generation_steps": []
        }
        
        input_ids = inputs.input_ids
        
        for step in range(max_new_tokens):
            with torch.no_grad():
                outputs = self.model(input_ids, output_attentions=True, use_cache=False)
            
            # è·å–ä¸‹ä¸€ä¸ªtoken
            next_token_logits = outputs.logits[0, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)
            next_token = self.tokenizer.convert_ids_to_tokens([next_token_id.item()])[0]
            
            # åˆ†ææ¯ä¸€å±‚å¯¹BOS tokençš„attention
            step_analysis = {
                "step": step,
                "generated_token": next_token,
                "bos_attention_by_layer": [],
                "total_attention_by_layer": [],
                "bos_attention_ratio_by_layer": []
            }
            
            current_seq_len = input_ids.shape[1]
            
            for layer_idx, layer_att in enumerate(outputs.attentions):
                # layer_att: (batch_size, num_heads, seq_len, seq_len)
                layer_att = layer_att[0]  # Remove batch dim
                
                # æœ€åä¸€ä¸ªtoken(å½“å‰ç”Ÿæˆçš„)çš„attentionåˆ†å¸ƒ
                last_token_att = layer_att[:, -1, :]  # (num_heads, seq_len)
                
                # å¯¹æ‰€æœ‰headæ±‚å¹³å‡
                avg_att = torch.mean(last_token_att, dim=0).cpu().numpy()  # (seq_len,)
                
                # BOS tokençš„attention (é€šå¸¸æ˜¯index 0)
                bos_attention = avg_att[0] if len(avg_att) > 0 else 0.0
                total_attention = np.sum(avg_att)
                bos_ratio = bos_attention / total_attention if total_attention > 0 else 0.0
                
                step_analysis["bos_attention_by_layer"].append(float(bos_attention))
                step_analysis["total_attention_by_layer"].append(float(total_attention))
                step_analysis["bos_attention_ratio_by_layer"].append(float(bos_ratio))
            
            bos_analysis["generation_steps"].append(step_analysis)
            
            # æ›´æ–°input_ids
            input_ids = torch.cat([input_ids, next_token_id], dim=1)
            
            print(f"Step {step+1}: {next_token}, BOS ratio in last layer: {step_analysis['bos_attention_ratio_by_layer'][-1]:.4f}")
        
        return bos_analysis
    
    def analyze_attention_distribution_patterns(self, context: str) -> Dict:
        """åˆ†æattentionåˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹æ€§"""
        print(f"ğŸ“Š Analyzing attention distribution patterns...")
        
        inputs = self.tokenizer(context, return_tensors="pt", padding=False).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs, output_attentions=True)
        
        context_tokens = self.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
        attentions = outputs.attentions
        
        analysis = {
            "context": context,
            "context_tokens": context_tokens,
            "num_layers": len(attentions),
            "layer_analysis": []
        }
        
        for layer_idx, layer_att in enumerate(attentions):
            # layer_att: (batch_size, num_heads, seq_len, seq_len)
            layer_att = layer_att[0].cpu().numpy()  # Remove batch dim
            
            # åˆ†æè¿™ä¸€å±‚çš„attentionæ¨¡å¼
            layer_stats = {
                "layer": layer_idx,
                "attention_entropy": [],  # æ¯ä¸ªä½ç½®çš„attentionç†µ
                "bos_attention_strength": [],  # æ¯ä¸ªä½ç½®å¯¹BOSçš„attention
                "attention_concentration": []  # attentionçš„é›†ä¸­åº¦
            }
            
            seq_len = layer_att.shape[-1]
            
            for pos in range(seq_len):
                # è¿™ä¸ªä½ç½®å¯¹æ‰€æœ‰ä¹‹å‰ä½ç½®çš„attention (causal mask)
                pos_attention = np.mean(layer_att[:, pos, :pos+1], axis=0)  # å¹³å‡æ‰€æœ‰head
                
                if len(pos_attention) > 0 and np.sum(pos_attention) > 0:
                    # è®¡ç®—ç†µ (attentionåˆ†å¸ƒçš„å‡åŒ€ç¨‹åº¦)
                    normalized_att = pos_attention / np.sum(pos_attention)
                    entropy = -np.sum(normalized_att * np.log(normalized_att + 1e-10))
                    
                    # BOS attentionå¼ºåº¦
                    bos_strength = pos_attention[0] if len(pos_attention) > 0 else 0.0
                    
                    # æ³¨æ„åŠ›é›†ä¸­åº¦ (æœ€å¤§attentionå€¼)
                    concentration = np.max(pos_attention)
                    
                    layer_stats["attention_entropy"].append(float(entropy))
                    layer_stats["bos_attention_strength"].append(float(bos_strength))
                    layer_stats["attention_concentration"].append(float(concentration))
            
            analysis["layer_analysis"].append(layer_stats)
        
        return analysis
    
    def visualize_bos_attention_evolution(self, bos_analysis: Dict):
        """å¯è§†åŒ–BOS token attentionåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¼”åŒ–"""
        steps = len(bos_analysis["generation_steps"])
        layers = len(bos_analysis["generation_steps"][0]["bos_attention_ratio_by_layer"])
        
        # æ„å»ºçŸ©é˜µ: (steps, layers)
        bos_ratio_matrix = np.zeros((steps, layers))
        
        for step_idx, step_data in enumerate(bos_analysis["generation_steps"]):
            bos_ratio_matrix[step_idx, :] = step_data["bos_attention_ratio_by_layer"]
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(14, 8))
        
        # ä½¿ç”¨å¯¹æ•°å°ºåº¦
        log_matrix = np.log1p(bos_ratio_matrix)
        
        im = plt.imshow(log_matrix.T, aspect='auto', cmap='viridis', interpolation='nearest')
        
        plt.title('BOS Token Attention Ratio Evolution (Log Scale)\nY-axis: Model Layers, X-axis: Generation Steps', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('Generation Steps', fontsize=12)
        plt.ylabel('Model Layers', fontsize=12)
        
        # è®¾ç½®xè½´æ ‡ç­¾
        generated_tokens = [step["generated_token"] for step in bos_analysis["generation_steps"]]
        x_labels = [f"Step{i+1}\n{token[:6]}" for i, token in enumerate(generated_tokens)]
        plt.xticks(range(len(x_labels)), x_labels, rotation=45, ha='right')
        
        # è®¾ç½®yè½´æ ‡ç­¾
        plt.yticks(range(0, layers, max(1, layers//10)), 
                  [f"Layer {i}" for i in range(0, layers, max(1, layers//10))])
        
        # é¢œè‰²æ¡
        cbar = plt.colorbar(im)
        cbar.set_label('Log(1 + BOS Attention Ratio)', rotation=270, labelpad=20)
        
        plt.tight_layout()
        plt.savefig('bos_attention_evolution.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š BOS attention evolution saved as bos_attention_evolution.png")
    
    def investigate_bos_anomaly(self, context: str) -> Dict:
        """ç»¼åˆè°ƒæŸ¥BOS token attentionå¼‚å¸¸çš„å®Œæ•´åˆ†æ"""
        print("ğŸ•µï¸ Starting comprehensive BOS anomaly investigation...")
        
        # 1. BOS attentionåˆ†æ
        bos_analysis = self.analyze_bos_token_attention(context, max_new_tokens=8)
        
        # 2. æ•´ä½“attentionåˆ†å¸ƒåˆ†æ
        distribution_analysis = self.analyze_attention_distribution_patterns(context)
        
        # 3. å¯è§†åŒ–
        self.visualize_bos_attention_evolution(bos_analysis)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report = {
            "investigation_summary": {
                "context": context,
                "bos_token_detected": bos_analysis["bos_token_index"] >= 0,
                "avg_bos_ratio_last_layer": np.mean([
                    step["bos_attention_ratio_by_layer"][-1] 
                    for step in bos_analysis["generation_steps"]
                ]),
                "max_bos_ratio_last_layer": np.max([
                    step["bos_attention_ratio_by_layer"][-1] 
                    for step in bos_analysis["generation_steps"]
                ])
            },
            "bos_analysis": bos_analysis,
            "distribution_analysis": distribution_analysis
        }
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        with open("bos_anomaly_investigation.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("ğŸ“‹ Investigation complete! Report saved as bos_anomaly_investigation.json")
        
        return report

def run_anomaly_investigation():
    """è¿è¡Œå®Œæ•´çš„attentionå¼‚å¸¸è°ƒæŸ¥"""
    print("ğŸ•µï¸ Starting MemoTree Attention Anomaly Investigation")
    print("=" * 60)
    
    analyzer = AttentionAnomalyAnalyzer()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_context = "MemoTree is a cognitive context management system for LLM agents."
    
    # è¿è¡Œè°ƒæŸ¥
    report = analyzer.investigate_bos_anomaly(test_context)
    
    # è¾“å‡ºå…³é”®å‘ç°
    summary = report["investigation_summary"]
    print("\nğŸ” Key Findings:")
    print(f"  BOS token detected: {summary['bos_token_detected']}")
    print(f"  Average BOS ratio (last layer): {summary['avg_bos_ratio_last_layer']:.4f}")
    print(f"  Maximum BOS ratio (last layer): {summary['max_bos_ratio_last_layer']:.4f}")
    
    return report

if __name__ == "__main__":
    report = run_anomaly_investigation()
