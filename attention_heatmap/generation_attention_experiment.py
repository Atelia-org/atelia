#!/usr/bin/env python3
"""
MemoTree Generation Attention Experiment
åŸºäºLLM generationé˜¶æ®µattentionæƒé‡çš„åŠ¨æ€çƒ­åŠ›å›¾åˆ†æ

æ ¸å¿ƒåˆ›æ–°ï¼šåˆ†ææ¯ä¸ªè¾“å‡ºtokenå¯¹è¾“å…¥contextçš„1D attentionåˆ†å¸ƒ
æ„å»º2Dé¢‘è°±å›¾ï¼šYè½´=è¾“å…¥tokensï¼ŒXè½´=è¾“å‡ºtokens
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Tuple, Optional
import json
from pathlib import Path
import matplotlib.pyplot as plt

class GenerationAttentionExtractor:
    """ä»LLM generationé˜¶æ®µæå–åŠ¨æ€attentionçš„æ ¸å¿ƒç±»"""
    
    # def __init__(self, model_path: str = r"W:\LLM\Llama-3.2-3B-Instruct"):
    def __init__(self, model_path: str = r"W:\LLM\Qwen3-4B"):
        print(f"ğŸš€ Loading model from {model_path}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“± Using device: {self.device}")
        
        # åŠ è½½tokenizerå’Œmodel
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            attn_implementation="eager"  # å¼ºåˆ¶ä½¿ç”¨eager attentionä»¥æ”¯æŒoutput_attentions
        )
        
        # ç¡®ä¿pad_tokenå­˜åœ¨
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"âœ… Model loaded successfully!")
        print(f"ğŸ“Š Model layers: {self.model.config.num_hidden_layers}")
        print(f"ğŸ”¢ Attention heads: {self.model.config.num_attention_heads}")
    
    def extract_generation_attention(self, context: str, max_new_tokens: int = 20, 
                                   max_context_length: int = 512) -> Dict:
        """æå–generationé˜¶æ®µæ¯ä¸ªè¾“å‡ºtokenå¯¹contextçš„attentionåˆ†å¸ƒ"""
        print(f"ğŸ” Extracting generation attention for context: {context[:100]}...")
        
        # Tokenize context
        context_inputs = self.tokenizer(
            context, 
            return_tensors="pt", 
            max_length=max_context_length,
            truncation=True,
            padding=False
        ).to(self.device)
        
        context_tokens = self.tokenizer.convert_ids_to_tokens(context_inputs.input_ids[0])
        # context_tokens = self.tokenizer.convert_ids_to_tokens(context_inputs.input_ids[0][1:]) # å®éªŒå»æ‰é¦–ä¸ª<|begin_of_text|>ã€‚ç»“æœæ˜¯æ–°çš„é¦–ä¸ªtokenåˆè·å¾—äº†ç›¸ä¼¼ç¨‹åº¦çš„æƒé‡ã€‚ä¸¤æ¬¡è¾“å‡ºçš„tokenåºåˆ—å®Œå…¨ç›¸åŒã€‚
        context_length = len(context_tokens)
        
        print(f"ğŸ“ Context length: {context_length} tokens")
        print(f"ğŸ¯ Generating {max_new_tokens} tokens...")
        
        # å­˜å‚¨æ¯ä¸ªç”Ÿæˆtokençš„attention
        generation_attentions = []  # List[Dict] - æ¯ä¸ªç”Ÿæˆtokençš„attentionæ•°æ®
        generated_tokens = []
        
        # åˆå§‹è¾“å…¥
        input_ids = context_inputs.input_ids
        
        for step in range(max_new_tokens):
            print(f"ğŸ”„ Generation step {step + 1}/{max_new_tokens}")
            
            # å‰å‘ä¼ æ’­è·å–ä¸‹ä¸€ä¸ªtokenå’Œattention
            with torch.no_grad():
                outputs = self.model(input_ids, output_attentions=True, use_cache=False)
            
            # è·å–ä¸‹ä¸€ä¸ªtoken
            next_token_logits = outputs.logits[0, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)
            next_token = self.tokenizer.convert_ids_to_tokens([next_token_id.item()])[0]
            
            # æå–å½“å‰ç”Ÿæˆtokenå¯¹æ‰€æœ‰ä¹‹å‰tokençš„attention
            # attentions: (num_layers, batch_size, num_heads, seq_len, seq_len)
            current_attentions = outputs.attentions
            current_seq_len = input_ids.shape[1]
            
            # æå–æœ€åä¸€ä¸ªä½ç½®(å½“å‰ç”Ÿæˆtoken)å¯¹contextéƒ¨åˆ†çš„attention
            token_attention_data = {
                "step": step,
                "generated_token": next_token,
                "generated_token_id": next_token_id.item(),
                "context_attention": []  # æ¯å±‚æ¯å¤´å¯¹contextçš„attention
            }
            
            for layer_idx, layer_att in enumerate(current_attentions):
                # layer_att: (batch_size, num_heads, seq_len, seq_len)
                layer_att = layer_att[0]  # Remove batch dim: (num_heads, seq_len, seq_len)
                
                # æå–æœ€åä¸€ä¸ªtoken(å½“å‰ç”Ÿæˆçš„)å¯¹contextéƒ¨åˆ†çš„attention
                last_token_att = layer_att[:, -1, :context_length]  # (num_heads, context_length)
                
                # å¯¹æ‰€æœ‰headæ±‚å¹³å‡å¾—åˆ°è¿™ä¸€å±‚å¯¹contextçš„attentionåˆ†å¸ƒ
                layer_context_att = torch.mean(last_token_att, dim=0).cpu().numpy()  # (context_length,)
                
                token_attention_data["context_attention"].append(layer_context_att)
            
            generation_attentions.append(token_attention_data)
            generated_tokens.append(next_token)
            
            # æ›´æ–°input_idsç”¨äºä¸‹ä¸€æ­¥ç”Ÿæˆ
            input_ids = torch.cat([input_ids, next_token_id], dim=1)
            
            # æ—©åœæ¡ä»¶
            if next_token_id.item() == self.tokenizer.eos_token_id:
                print(f"ğŸ›‘ EOS token generated at step {step + 1}")
                break
        
        print(f"âœ… Generated {len(generated_tokens)} tokens: {generated_tokens}")
        
        return {
            "context": context,
            "context_tokens": context_tokens,
            "context_length": context_length,
            "generated_tokens": generated_tokens,
            "generation_attentions": generation_attentions,
            "total_steps": len(generation_attentions)
        }
    
    def visualize_generation_attention_spectrogram(self, generation_data: Dict, 
                                                  layer_idx: int = -1, title_suffix: str = ""):
        """å¯è§†åŒ–generationé˜¶æ®µçš„attentioné¢‘è°±å›¾ - ç±»ä¼¼åˆ˜ä¸–è¶…æè¿°çš„2Då›¾"""
        context_tokens = generation_data["context_tokens"]
        generation_attentions = generation_data["generation_attentions"]
        generated_tokens = generation_data["generated_tokens"]
        
        if not generation_attentions:
            print("âš ï¸ No generation attention data")
            return
        
        # æ„å»º2D attentionçŸ©é˜µ: (ç”Ÿæˆæ­¥æ•°, contexté•¿åº¦)
        num_steps = len(generation_attentions)
        context_length = len(context_tokens)
        
        attention_matrix = np.zeros((num_steps, context_length))
        
        for step_idx, step_data in enumerate(generation_attentions):
            context_attention = step_data["context_attention"]
            
            if layer_idx == -1:
                # ä½¿ç”¨æœ€åä¸€å±‚
                layer_attention = context_attention[-1]
            else:
                # ä½¿ç”¨æŒ‡å®šå±‚
                layer_attention = context_attention[layer_idx] if layer_idx < len(context_attention) else context_attention[-1]
            
            
            attention_matrix[step_idx, :] = layer_attention
            # attention_matrix[step_idx, 1:] = layer_attention[1:] # å®éªŒæ’é™¤åˆå§‹æƒé‡é«˜çš„ç¦»è°±çš„<|begin_of_text|>
        
        # åˆ›å»ºé¢‘è°±å›¾ - ä½¿ç”¨å¯¹æ•°å°ºåº¦é¿å…è¿‡æ›
        plt.figure(figsize=(16, 10))

        # å¯¹attentionçŸ©é˜µåº”ç”¨å¯¹æ•°å˜æ¢ï¼Œé¿å…<|begin_of_text|>è¿‡æ›
        log_attention_matrix = np.log1p(attention_matrix)  # log1p = log(1+x) é¿å…log(0)

        # ä½¿ç”¨imshowåˆ›å»ºçƒ­åŠ›å›¾
        im = plt.imshow(log_attention_matrix.T, aspect='auto', cmap='hot', interpolation='nearest')
        
        # è®¾ç½®æ ‡ç­¾
        plt.title(f'MemoTree Generation Attention Spectrogram (Log Scale){title_suffix}\n'
                 f'Y-axis: Context Tokens, X-axis: Generated Tokens',
                 fontsize=14, fontweight='bold')
        plt.xlabel('Generation Steps', fontsize=12)
        plt.ylabel('Context Tokens', fontsize=12)
        
        # è®¾ç½®xè½´æ ‡ç­¾(ç”Ÿæˆçš„tokens)
        x_labels = [f"Step{i+1}\n{token[:8]}" for i, token in enumerate(generated_tokens)]
        plt.xticks(range(len(x_labels)), x_labels, rotation=45, ha='right')
        
        # è®¾ç½®yè½´æ ‡ç­¾(context tokens) - åªæ˜¾ç¤ºéƒ¨åˆ†ä»¥é¿å…è¿‡äºå¯†é›†
        y_step = max(1, context_length // 20)  # æœ€å¤šæ˜¾ç¤º20ä¸ªæ ‡ç­¾
        y_indices = range(0, context_length, y_step)
        y_labels = [context_tokens[i][:10] for i in y_indices]
        plt.yticks(y_indices, y_labels)
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im)
        cbar.set_label('Log(1 + Attention Weight)', rotation=270, labelpad=20)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = f"generation_attention_spectrogram{title_suffix.replace(' ', '_').lower()}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"ğŸ“Š Spectrogram saved as {filename}")
        
        return attention_matrix
    
    def analyze_dynamic_attention_patterns(self, generation_data: Dict) -> Dict:
        """åˆ†æåŠ¨æ€attentionæ¨¡å¼ - è¿™æ˜¯MemoTree LODè°ƒæ•´çš„æ ¸å¿ƒ"""
        context_tokens = generation_data["context_tokens"]
        generation_attentions = generation_data["generation_attentions"]
        
        print("ğŸ” Analyzing dynamic attention patterns...")
        
        # åˆ†ææ¯ä¸ªcontext tokenåœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­çš„attentionå˜åŒ–
        context_attention_evolution = {}
        
        for token_idx, token in enumerate(context_tokens):
            token_attention_over_time = []
            
            for step_data in generation_attentions:
                # ä½¿ç”¨æœ€åä¸€å±‚çš„attention
                last_layer_attention = step_data["context_attention"][-1]
                token_attention_over_time.append(last_layer_attention[token_idx])
            
            context_attention_evolution[f"token_{token_idx}_{token}"] = {
                "attention_sequence": token_attention_over_time,
                "max_attention": max(token_attention_over_time),
                "min_attention": min(token_attention_over_time),
                "attention_variance": np.var(token_attention_over_time),
                "final_attention": token_attention_over_time[-1] if token_attention_over_time else 0
            }
        
        # è¯†åˆ«é«˜åŠ¨æ€æ€§çš„tokens(attentionå˜åŒ–å¤§çš„)
        high_dynamic_tokens = sorted(
            context_attention_evolution.items(),
            key=lambda x: x[1]["attention_variance"],
            reverse=True
        )[:5]
        
        # è¯†åˆ«æŒç»­é«˜attentionçš„tokens
        high_attention_tokens = sorted(
            context_attention_evolution.items(),
            key=lambda x: x[1]["max_attention"],
            reverse=True
        )[:5]
        
        print("ğŸ¯ Top 5 High Dynamic Tokens (å˜åŒ–æœ€å¤§):")
        for token_key, stats in high_dynamic_tokens:
            token_name = token_key.split("_")[-1]
            print(f"  {token_name}: variance={stats['attention_variance']:.4f}, "
                  f"max={stats['max_attention']:.4f}")
        
        print("ğŸ”¥ Top 5 High Attention Tokens (æ³¨æ„åŠ›æœ€é«˜):")
        for token_key, stats in high_attention_tokens:
            token_name = token_key.split("_")[-1]
            print(f"  {token_name}: max={stats['max_attention']:.4f}, "
                  f"final={stats['final_attention']:.4f}")
        
        return {
            "context_attention_evolution": context_attention_evolution,
            "high_dynamic_tokens": high_dynamic_tokens,
            "high_attention_tokens": high_attention_tokens
        }

def run_generation_experiment():
    """è¿è¡Œgenerationé˜¶æ®µattentionåˆ†æå®éªŒ"""
    print("ğŸ§ª Starting MemoTree Generation Attention Experiment")
    print("=" * 60)
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = GenerationAttentionExtractor()
    
    # æµ‹è¯•contextï¼šæ¨¡æ‹ŸMemoTreeçš„è®¤çŸ¥ä¸Šä¸‹æ–‡
    context = """MemoTree is a cognitive context management system for LLM agents. It uses hierarchical LOD (Level of Detail) nodes that can be dynamically expanded or collapsed based on attention patterns. The system integrates with Git for version control and supports semantic relationships between cognitive nodes."""
    
    print(f"ğŸ“ Context: {context}")
    print()
    
    # Step 1: æå–generationé˜¶æ®µçš„attention
    print("ğŸš€ Extracting generation attention...")
    generation_data = extractor.extract_generation_attention(
        context=context, 
        max_new_tokens=15,
        max_context_length=256
    )
    
    print(f"âœ… Generated tokens: {generation_data['generated_tokens']}")
    print()
    
    # Step 2: å¯è§†åŒ–attentioné¢‘è°±å›¾
    print("ğŸ“Š Creating attention spectrogram...")
    attention_matrix = extractor.visualize_generation_attention_spectrogram(
        generation_data, 
        layer_idx=0,  # ä½¿ç”¨ç¬¬ä¸€å±‚
        title_suffix=" - First Layer"
    )
    
    # Step 3: åˆ†æåŠ¨æ€attentionæ¨¡å¼
    print("ğŸ” Analyzing dynamic patterns...")
    pattern_analysis = extractor.analyze_dynamic_attention_patterns(generation_data)
    
    return generation_data, attention_matrix, pattern_analysis

if __name__ == "__main__":
    # è¿è¡Œgenerationå®éªŒ
    generation_data, attention_matrix, pattern_analysis = run_generation_experiment()
