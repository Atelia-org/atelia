#!/usr/bin/env python3
"""
MemoTree Attention-Driven LOD Experiment
åŸºäºLLMå†…åœ¨attentionæƒé‡çš„åŠ¨æ€çƒ­åŠ›å›¾ç”Ÿæˆå®éªŒ

ç›®æ ‡ï¼šéªŒè¯ä»æ¨¡å‹attentionå±‚æå–æƒé‡å¹¶æ˜ å°„åˆ°è®¤çŸ¥èŠ‚ç‚¹çš„å¯è¡Œæ€§
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Tuple, Optional
import json
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

class AttentionHeatmapExtractor:
    """ä»LLM attentionå±‚æå–çƒ­åŠ›å›¾çš„æ ¸å¿ƒç±»"""
    
    def __init__(self, model_path: str = r"W:\LLM\Llama-3.2-3B-Instruct"):
        print(f"ğŸš€ Loading model from {model_path}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“± Using device: {self.device}")
        
        # åŠ è½½tokenizerå’Œmodel
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            output_attentions=True  # å…³é”®ï¼šå¯ç”¨attentionè¾“å‡º
        )
        
        # ç¡®ä¿pad_tokenå­˜åœ¨
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"âœ… Model loaded successfully!")
        print(f"ğŸ“Š Model layers: {self.model.config.num_hidden_layers}")
        print(f"ğŸ”¢ Attention heads: {self.model.config.num_attention_heads}")
    
    def extract_generation_attention(self, context: str, max_new_tokens: int = 20,
                                   max_context_length: int = 512) -> Dict:
        """æå–generationé˜¶æ®µæ¯ä¸ªè¾“å‡ºtokenå¯¹contextçš„attentionåˆ†å¸ƒ"""
        print(f"ğŸ” Extracting generation attention for context: {context[:100]}...")

        # Tokenize context
        context_inputs = self.tokenizer(
            context,
            return_tensors="pt",
            max_length=max_context_length,
            truncation=True,
            padding=False
        ).to(self.device)

        context_tokens = self.tokenizer.convert_ids_to_tokens(context_inputs.input_ids[0])
        context_length = len(context_tokens)

        print(f"ğŸ“ Context length: {context_length} tokens")
        print(f"ğŸ¯ Generating {max_new_tokens} tokens...")

        # å­˜å‚¨æ¯ä¸ªç”Ÿæˆtokençš„attention
        generation_attentions = []  # List[Dict] - æ¯ä¸ªç”Ÿæˆtokençš„attentionæ•°æ®
        generated_tokens = []

        # åˆå§‹è¾“å…¥
        input_ids = context_inputs.input_ids

        for step in range(max_new_tokens):
            print(f"ğŸ”„ Generation step {step + 1}/{max_new_tokens}")

            # å‰å‘ä¼ æ’­è·å–ä¸‹ä¸€ä¸ªtokenå’Œattention
            with torch.no_grad():
                outputs = self.model(input_ids, output_attentions=True, use_cache=True)

            # è·å–ä¸‹ä¸€ä¸ªtoken
            next_token_logits = outputs.logits[0, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)
            next_token = self.tokenizer.convert_ids_to_tokens([next_token_id.item()])[0]

            # æå–å½“å‰ç”Ÿæˆtokenå¯¹æ‰€æœ‰ä¹‹å‰tokençš„attention
            # attentions: (num_layers, batch_size, num_heads, seq_len, seq_len)
            current_attentions = outputs.attentions
            current_seq_len = input_ids.shape[1]

            # æå–æœ€åä¸€ä¸ªä½ç½®(å½“å‰ç”Ÿæˆtoken)å¯¹contextéƒ¨åˆ†çš„attention
            token_attention_data = {
                "step": step,
                "generated_token": next_token,
                "generated_token_id": next_token_id.item(),
                "context_attention": []  # æ¯å±‚æ¯å¤´å¯¹contextçš„attention
            }

            for layer_idx, layer_att in enumerate(current_attentions):
                # layer_att: (batch_size, num_heads, seq_len, seq_len)
                layer_att = layer_att[0]  # Remove batch dim: (num_heads, seq_len, seq_len)

                # æå–æœ€åä¸€ä¸ªtoken(å½“å‰ç”Ÿæˆçš„)å¯¹contextéƒ¨åˆ†çš„attention
                last_token_att = layer_att[:, -1, :context_length]  # (num_heads, context_length)

                # å¯¹æ‰€æœ‰headæ±‚å¹³å‡å¾—åˆ°è¿™ä¸€å±‚å¯¹contextçš„attentionåˆ†å¸ƒ
                layer_context_att = torch.mean(last_token_att, dim=0).cpu().numpy()  # (context_length,)

                token_attention_data["context_attention"].append(layer_context_att)

            generation_attentions.append(token_attention_data)
            generated_tokens.append(next_token)

            # æ›´æ–°input_idsç”¨äºä¸‹ä¸€æ­¥ç”Ÿæˆ
            input_ids = torch.cat([input_ids, next_token_id], dim=1)

            # æ—©åœæ¡ä»¶
            if next_token_id.item() == self.tokenizer.eos_token_id:
                print(f"ğŸ›‘ EOS token generated at step {step + 1}")
                break

        print(f"âœ… Generated {len(generated_tokens)} tokens: {generated_tokens}")

        return {
            "context": context,
            "context_tokens": context_tokens,
            "context_length": context_length,
            "generated_tokens": generated_tokens,
            "generation_attentions": generation_attentions,
            "total_steps": len(generation_attentions)
        }
    
    def aggregate_attention_to_heatmap(self, attention_data: Dict, 
                                     aggregation_method: str = "mean_last_layer") -> Dict[str, float]:
        """å°†attentionæƒé‡èšåˆä¸ºtokençº§åˆ«çš„çƒ­åŠ›å›¾"""
        tokens = attention_data["tokens"]
        attentions = attention_data["attentions"]
        
        if aggregation_method == "mean_last_layer":
            # ä½¿ç”¨æœ€åä¸€å±‚çš„å¹³å‡attention
            last_layer_att = attentions[-1]  # (num_heads, seq_len, seq_len)
            # å¯¹æ‰€æœ‰headæ±‚å¹³å‡ï¼Œç„¶åå¯¹æ¯ä¸ªtokençš„incoming attentionæ±‚å’Œ
            token_attention = np.mean(last_layer_att, axis=0).sum(axis=0)
            
        elif aggregation_method == "max_across_layers":
            # è·¨å±‚å–æœ€å¤§attention
            all_layers_att = np.stack([np.mean(att, axis=0).sum(axis=0) for att in attentions])
            token_attention = np.max(all_layers_att, axis=0)
            
        elif aggregation_method == "weighted_layers":
            # ç»™åé¢çš„å±‚æ›´é«˜æƒé‡
            weights = np.linspace(0.1, 1.0, len(attentions))
            weighted_att = []
            for i, att in enumerate(attentions):
                layer_att = np.mean(att, axis=0).sum(axis=0) * weights[i]
                weighted_att.append(layer_att)
            token_attention = np.sum(weighted_att, axis=0)
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        if token_attention.max() > 0:
            token_attention = token_attention / token_attention.max()
        
        # æ„å»ºtokenåˆ°attentionæƒé‡çš„æ˜ å°„
        heatmap = {}
        for i, token in enumerate(tokens):
            if i < len(token_attention):
                heatmap[f"token_{i}_{token}"] = float(token_attention[i])
        
        return heatmap
    
    def simulate_concept_mapping(self, heatmap: Dict[str, float], 
                               concept_nodes: List[str]) -> Dict[str, float]:
        """æ¨¡æ‹Ÿå°†tokençº§attentionæ˜ å°„åˆ°æ¦‚å¿µèŠ‚ç‚¹çš„è¿‡ç¨‹"""
        print("ğŸ—ºï¸ Simulating token-to-concept mapping...")
        
        # ç®€åŒ–çš„æ¦‚å¿µæ˜ å°„ï¼šåŸºäºå…³é”®è¯åŒ¹é…
        concept_attention = {concept: 0.0 for concept in concept_nodes}
        
        for token_key, attention_weight in heatmap.items():
            token = token_key.split("_")[-1].lower().strip("â–")  # å¤„ç†SentencePiece token
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…ç­–ç•¥
            for concept in concept_nodes:
                if token in concept.lower() or concept.lower() in token:
                    concept_attention[concept] += attention_weight
                    
        # å½’ä¸€åŒ–
        max_attention = max(concept_attention.values()) if concept_attention.values() else 1.0
        if max_attention > 0:
            concept_attention = {k: v/max_attention for k, v in concept_attention.items()}
            
        return concept_attention
    
    def visualize_heatmap(self, heatmap: Dict[str, float], title: str = "Attention Heatmap"):
        """å¯è§†åŒ–attentionçƒ­åŠ›å›¾"""
        if not heatmap:
            print("âš ï¸ Empty heatmap, skipping visualization")
            return
            
        # å‡†å¤‡æ•°æ®
        items = list(heatmap.keys())
        values = list(heatmap.values())
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 8))
        colors = plt.cm.Reds(np.array(values))
        
        bars = plt.bar(range(len(items)), values, color=colors)
        plt.title(title, fontsize=16, fontweight='bold')
        plt.xlabel("Tokens/Concepts", fontsize=12)
        plt.ylabel("Attention Weight", fontsize=12)
        
        # è®¾ç½®xè½´æ ‡ç­¾
        plt.xticks(range(len(items)), [item.split("_")[-1][:10] for item in items], 
                  rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars, values)):
            if value > 0.1:  # åªæ˜¾ç¤ºè¾ƒé«˜çš„å€¼
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                        f'{value:.2f}', ha='center', va='bottom', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(f"attention_heatmap_{title.replace(' ', '_').lower()}.png", 
                   dpi=300, bbox_inches='tight')
        plt.show()
        print(f"ğŸ“Š Heatmap saved as attention_heatmap_{title.replace(' ', '_').lower()}.png")

def run_experiment():
    """è¿è¡Œå®Œæ•´çš„attentionçƒ­åŠ›å›¾æå–å®éªŒ"""
    print("ğŸ§ª Starting MemoTree Attention-Driven LOD Experiment")
    print("=" * 60)
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = AttentionHeatmapExtractor()
    
    # æµ‹è¯•æ–‡æœ¬ï¼šæ¨¡æ‹Ÿå¤æ‚çš„è®¤çŸ¥ä»»åŠ¡
    test_text = """
    In the context of artificial intelligence and cognitive architectures, 
    the concept of attention mechanisms plays a crucial role in determining 
    which information should be prioritized during processing. Memory systems 
    in AI agents need to dynamically adjust their level of detail based on 
    the current cognitive focus, similar to how human attention works.
    """
    
    # æ¨¡æ‹Ÿçš„æ¦‚å¿µèŠ‚ç‚¹ï¼ˆåœ¨çœŸå®MemoTreeä¸­è¿™äº›æ¥è‡ªè®¤çŸ¥å›¾è°±ï¼‰
    concept_nodes = [
        "Artificial Intelligence",
        "Cognitive Architecture", 
        "Attention Mechanisms",
        "Memory Systems",
        "Information Processing",
        "AI Agents",
        "Human Cognition"
    ]
    
    print(f"ğŸ¯ Test concepts: {concept_nodes}")
    print()
    
    # Step 1: æå–attentionæƒé‡
    attention_data = extractor.extract_attention_weights(test_text)
    
    # Step 2: èšåˆä¸ºçƒ­åŠ›å›¾
    print("ğŸ”¥ Generating attention heatmaps...")
    heatmaps = {}
    
    for method in ["mean_last_layer", "max_across_layers", "weighted_layers"]:
        print(f"ğŸ“Š Using aggregation method: {method}")
        heatmap = extractor.aggregate_attention_to_heatmap(attention_data, method)
        heatmaps[method] = heatmap
        
        # æ˜¾ç¤ºtop-5 tokens
        top_tokens = sorted(heatmap.items(), key=lambda x: x[1], reverse=True)[:5]
        print(f"ğŸ” Top 5 tokens: {top_tokens}")
        print()
    
    # Step 3: æ˜ å°„åˆ°æ¦‚å¿µèŠ‚ç‚¹
    print("ğŸ—ºï¸ Mapping to concept nodes...")
    concept_heatmaps = {}
    
    for method, token_heatmap in heatmaps.items():
        concept_heatmap = extractor.simulate_concept_mapping(token_heatmap, concept_nodes)
        concept_heatmaps[method] = concept_heatmap
        
        print(f"ğŸ“ˆ {method} - Concept attention:")
        for concept, attention in sorted(concept_heatmap.items(), key=lambda x: x[1], reverse=True):
            if attention > 0:
                print(f"  {concept}: {attention:.3f}")
        print()
    
    # Step 4: å¯è§†åŒ–
    print("ğŸ“Š Generating visualizations...")
    for method, concept_heatmap in concept_heatmaps.items():
        extractor.visualize_heatmap(concept_heatmap, f"Concept Attention - {method}")
    
    # Step 5: ä¿å­˜ç»“æœ
    results = {
        "test_text": test_text,
        "concept_nodes": concept_nodes,
        "token_heatmaps": heatmaps,
        "concept_heatmaps": concept_heatmaps,
        "model_info": {
            "model_path": extractor.model.config.name_or_path,
            "num_layers": extractor.model.config.num_hidden_layers,
            "num_heads": extractor.model.config.num_attention_heads
        }
    }
    
    with open("attention_experiment_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("ğŸ’¾ Results saved to attention_experiment_results.json")
    print("ğŸ‰ Experiment completed successfully!")
    
    return results

if __name__ == "__main__":
    results = run_experiment()
