#!/usr/bin/env python3
"""
Attention Attribution Analyzer
å¤šå±‚attentionå½’å› åˆ†æ - ç±»ä¼¼PageRankçš„é€å±‚å‘å‰å½’å› 

æ ¸å¿ƒæ€æƒ³ï¼š
1. æ¯ä¸€å±‚çš„attentionä¸æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œæ˜¯åŸºäºå‰ä¸€å±‚çš„è¡¨å¾
2. éœ€è¦é€šè¿‡çŸ©é˜µä¹˜æ³•é“¾å¼å½’å› åˆ°åŸå§‹token
3. ç±»ä¼¼PageRankç®—æ³•ï¼Œè®¡ç®—tokençš„"é‡è¦æ€§ä¼ æ’­"
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import json

class AttentionAttributionAnalyzer:
    """å¤šå±‚attentionå½’å› åˆ†æå™¨"""
    
    # def __init__(self, model_path: str = r"W:\LLM\Llama-3.2-3B-Instruct"):
    def __init__(self, model_path: str = r"W:\LLM\Qwen3-4B"):
        print(f"ğŸ”— Loading model for attribution analysis: {model_path}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            attn_implementation="eager"
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"âœ… Attribution analyzer ready")
    
    def extract_layered_attention_matrices(self, context: str) -> Dict:
        """æå–æ‰€æœ‰å±‚çš„attentionçŸ©é˜µç”¨äºå½’å› åˆ†æ"""
        print(f"ğŸ” Extracting layered attention matrices...")
        
        inputs = self.tokenizer(context, return_tensors="pt", padding=False).to(self.device)
        context_tokens = self.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
        
        with torch.no_grad():
            outputs = self.model(**inputs, output_attentions=True)
        
        # æå–æ‰€æœ‰å±‚çš„attentionçŸ©é˜µ
        attention_matrices = []
        for layer_att in outputs.attentions:
            # layer_att: (batch_size, num_heads, seq_len, seq_len)
            layer_att = layer_att[0].cpu().numpy()  # Remove batch dim
            
            # å¯¹æ‰€æœ‰headæ±‚å¹³å‡å¾—åˆ° (seq_len, seq_len) çŸ©é˜µ
            avg_layer_att = np.mean(layer_att, axis=0)
            attention_matrices.append(avg_layer_att)
        
        return {
            "context": context,
            "context_tokens": context_tokens,
            "attention_matrices": attention_matrices,  # List of (seq_len, seq_len) matrices
            "num_layers": len(attention_matrices),
            "seq_len": len(context_tokens)
        }
    
    def compute_attribution_via_matrix_chain(self, attention_data: Dict, 
                                           target_position: int = -1) -> Dict:
        """é€šè¿‡çŸ©é˜µé“¾ä¹˜æ³•è®¡ç®—å½’å›  - ç±»ä¼¼PageRank"""
        print(f"ğŸ”— Computing attribution via matrix chain multiplication...")
        
        attention_matrices = attention_data["attention_matrices"]
        context_tokens = attention_data["context_tokens"]
        seq_len = attention_data["seq_len"]
        
        if target_position == -1:
            target_position = seq_len - 1  # æœ€åä¸€ä¸ªtoken
        
        print(f"ğŸ¯ Target position: {target_position} (token: {context_tokens[target_position]})")
        
        # æ–¹æ³•1: é€å±‚ç´¯ç§¯å½’å›  (ç±»ä¼¼PageRankçš„è¿­ä»£)
        # åˆå§‹åŒ–ï¼šç›®æ ‡ä½ç½®çš„å½’å› å‘é‡
        current_attribution = np.zeros(seq_len)
        current_attribution[target_position] = 1.0  # ç›®æ ‡tokenåˆå§‹æƒé‡ä¸º1
        
        # ä»æœ€åä¸€å±‚å‘å‰ä¼ æ’­å½’å› 
        layer_attributions = []
        
        for layer_idx in reversed(range(len(attention_matrices))):
            att_matrix = attention_matrices[layer_idx]
            
            # å½’å› ä¼ æ’­ï¼šå½“å‰å½’å›  = å‰ä¸€å±‚å½’å›  Ã— attentionçŸ©é˜µ
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è½¬ç½®ï¼Œå› ä¸ºæˆ‘ä»¬è¦è®¡ç®—"è°å¯¹å½“å‰tokenæœ‰è´¡çŒ®"
            new_attribution = np.dot(current_attribution, att_matrix)
            
            layer_attributions.append({
                "layer": layer_idx,
                "attribution_vector": new_attribution.copy(),
                "top_contributors": self._get_top_contributors(new_attribution, context_tokens, top_k=5)
            })
            
            current_attribution = new_attribution
            
            print(f"Layer {layer_idx}: Top contributor = {layer_attributions[-1]['top_contributors'][0]}")
        
        # åè½¬åˆ—è¡¨ï¼Œä½¿å…¶ä»ç¬¬0å±‚åˆ°æœ€åä¸€å±‚
        layer_attributions.reverse()
        
        # æ–¹æ³•2: ç›´æ¥çŸ©é˜µé“¾ä¹˜æ³• (æ‰€æœ‰å±‚çš„å¤åˆæ•ˆåº”)
        print("ğŸ”— Computing direct matrix chain multiplication...")
        
        # è®¡ç®—æ‰€æœ‰attentionçŸ©é˜µçš„ä¹˜ç§¯
        composite_matrix = np.eye(seq_len)  # å•ä½çŸ©é˜µ
        
        for att_matrix in attention_matrices:
            composite_matrix = np.dot(composite_matrix, att_matrix)
        
        # ç›®æ ‡ä½ç½®çš„æœ€ç»ˆå½’å› 
        final_attribution = composite_matrix[target_position, :]
        
        return {
            "target_position": target_position,
            "target_token": context_tokens[target_position],
            "layer_by_layer_attribution": layer_attributions,
            "composite_matrix": composite_matrix,
            "final_attribution": final_attribution,
            "final_top_contributors": self._get_top_contributors(final_attribution, context_tokens, top_k=10)
        }
    
    def _get_top_contributors(self, attribution_vector: np.ndarray, 
                            tokens: List[str], top_k: int = 5) -> List[Tuple[str, float, int]]:
        """è·å–è´¡çŒ®æœ€å¤§çš„tokens"""
        indexed_attributions = [(tokens[i], attribution_vector[i], i) 
                              for i in range(len(attribution_vector))]
        return sorted(indexed_attributions, key=lambda x: x[1], reverse=True)[:top_k]
    
    def visualize_attribution_evolution(self, attribution_result: Dict):
        """å¯è§†åŒ–å½’å› åœ¨å„å±‚çš„æ¼”åŒ–è¿‡ç¨‹"""
        layer_attributions = attribution_result["layer_by_layer_attribution"]
        context_tokens = [item[0] for item in attribution_result["final_top_contributors"]][:20]  # åªæ˜¾ç¤ºå‰20ä¸ª
        
        # æ„å»ºçŸ©é˜µ: (layers, tokens)
        num_layers = len(layer_attributions)
        num_tokens = len(context_tokens)
        
        attribution_matrix = np.zeros((num_layers, num_tokens))
        
        for layer_idx, layer_data in enumerate(layer_attributions):
            attribution_vector = layer_data["attribution_vector"]
            
            # åªå–å‰20ä¸ªtokençš„å½’å› å€¼
            for token_idx in range(min(num_tokens, len(attribution_vector))):
                attribution_matrix[layer_idx, token_idx] = attribution_vector[token_idx]
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(16, 10))
        
        # ä½¿ç”¨å¯¹æ•°å°ºåº¦
        log_matrix = np.log1p(np.abs(attribution_matrix))  # å–ç»å¯¹å€¼é¿å…è´Ÿæ•°
        
        im = plt.imshow(log_matrix, aspect='auto', cmap='plasma', interpolation='nearest')
        
        plt.title(f'Attribution Evolution Across Layers (Log Scale)\n'
                 f'Target: {attribution_result["target_token"]}', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('Context Tokens', fontsize=12)
        plt.ylabel('Model Layers', fontsize=12)
        
        # è®¾ç½®æ ‡ç­¾
        plt.xticks(range(num_tokens), [token[:8] for token in context_tokens], 
                  rotation=45, ha='right')
        plt.yticks(range(0, num_layers, max(1, num_layers//10)), 
                  [f"Layer {i}" for i in range(0, num_layers, max(1, num_layers//10))])
        
        # é¢œè‰²æ¡
        cbar = plt.colorbar(im)
        cbar.set_label('Log(1 + |Attribution|)', rotation=270, labelpad=20)
        
        plt.tight_layout()
        plt.savefig('attribution_evolution.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š Attribution evolution saved as attribution_evolution.png")
    
    def analyze_attribution_patterns(self, context: str, target_positions: List[int] = None) -> Dict:
        """åˆ†æå¤šä¸ªä½ç½®çš„å½’å› æ¨¡å¼"""
        print("ğŸ” Analyzing attribution patterns...")
        
        # æå–attentionçŸ©é˜µ
        attention_data = self.extract_layered_attention_matrices(context)
        
        if target_positions is None:
            # é»˜è®¤åˆ†ææœ€åå‡ ä¸ªtoken
            seq_len = attention_data["seq_len"]
            target_positions = list(range(max(0, seq_len-3), seq_len))
        
        results = {}
        
        for pos in target_positions:
            if pos < attention_data["seq_len"]:
                print(f"\nğŸ¯ Analyzing position {pos}...")
                attribution_result = self.compute_attribution_via_matrix_chain(
                    attention_data, target_position=pos
                )
                results[f"position_{pos}"] = attribution_result
                
                # å¯è§†åŒ–è¿™ä¸ªä½ç½®çš„å½’å› æ¼”åŒ–
                self.visualize_attribution_evolution(attribution_result)
        
        # ä¿å­˜ç»“æœ
        with open("attribution_analysis_results.json", "w", encoding="utf-8") as f:
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            serializable_results = {}
            for key, result in results.items():
                serializable_result = result.copy()
                serializable_result["final_attribution"] = result["final_attribution"].tolist()
                serializable_result["composite_matrix"] = result["composite_matrix"].tolist()
                
                # è½¬æ¢layer_by_layer_attribution
                for layer_data in serializable_result["layer_by_layer_attribution"]:
                    layer_data["attribution_vector"] = layer_data["attribution_vector"].tolist()
                
                serializable_results[key] = serializable_result
            
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ Attribution analysis saved as attribution_analysis_results.json")
        
        return results

def run_attribution_analysis():
    """è¿è¡Œå®Œæ•´çš„å½’å› åˆ†æ"""
    print("ğŸ”— Starting MemoTree Attribution Analysis")
    print("=" * 60)
    
    analyzer = AttentionAttributionAnalyzer()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_context = "MemoTree uses hierarchical LOD nodes for cognitive context management."
    
    # è¿è¡Œå½’å› åˆ†æ
    results = analyzer.analyze_attribution_patterns(test_context)
    
    # è¾“å‡ºå…³é”®å‘ç°
    print("\nğŸ” Attribution Analysis Summary:")
    for pos_key, result in results.items():
        print(f"\n{pos_key} (token: {result['target_token']}):")
        print("  Top 3 final contributors:")
        for i, (token, attribution, idx) in enumerate(result['final_top_contributors'][:3]):
            print(f"    {i+1}. {token}: {attribution:.6f}")
    
    return results

if __name__ == "__main__":
    results = run_attribution_analysis()
