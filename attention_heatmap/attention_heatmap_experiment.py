#!/usr/bin/env python3
"""
MemoTree Attention-Driven LOD Experiment
åŸºäºLLMå†…åœ¨attentionæƒé‡çš„åŠ¨æ€çƒ­åŠ›å›¾ç”Ÿæˆå®éªŒ

ç›®æ ‡ï¼šéªŒè¯ä»æ¨¡å‹attentionå±‚æå–æƒé‡å¹¶æ˜ å°„åˆ°è®¤çŸ¥èŠ‚ç‚¹çš„å¯è¡Œæ€§
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Tuple, Optional
import json
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

class AttentionHeatmapExtractor:
    """ä»LLM attentionå±‚æå–çƒ­åŠ›å›¾çš„æ ¸å¿ƒç±»"""
    
    def __init__(self, model_path: str = r"W:\LLM\Llama-3.2-3B-Instruct"):
        print(f"ğŸš€ Loading model from {model_path}")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“± Using device: {self.device}")
        
        # åŠ è½½tokenizerå’Œmodel
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            output_attentions=True  # å…³é”®ï¼šå¯ç”¨attentionè¾“å‡º
        )
        
        # ç¡®ä¿pad_tokenå­˜åœ¨
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"âœ… Model loaded successfully!")
        print(f"ğŸ“Š Model layers: {self.model.config.num_hidden_layers}")
        print(f"ğŸ”¢ Attention heads: {self.model.config.num_attention_heads}")
    
    def extract_attention_weights(self, text: str, max_length: int = 512) -> Dict:
        """æå–æ–‡æœ¬çš„attentionæƒé‡çŸ©é˜µ"""
        print(f"ğŸ” Extracting attention for text: {text[:100]}...")
        
        # Tokenizeè¾“å…¥
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            max_length=max_length,
            truncation=True,
            padding=True
        ).to(self.device)
        
        # å‰å‘ä¼ æ’­ï¼Œè·å–attentionæƒé‡
        with torch.no_grad():
            outputs = self.model(**inputs, output_attentions=True)
        
        # æå–attentionæƒé‡ (num_layers, batch_size, num_heads, seq_len, seq_len)
        attentions = outputs.attentions
        tokens = self.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
        
        print(f"ğŸ“ Sequence length: {len(tokens)}")
        print(f"ğŸ§  Attention layers: {len(attentions)}")
        
        return {
            "tokens": tokens,
            "input_ids": inputs.input_ids[0].cpu().numpy(),
            "attentions": [att[0].cpu().numpy() for att in attentions],  # Remove batch dim
            "text": text
        }
    
    def aggregate_attention_to_heatmap(self, attention_data: Dict, 
                                     aggregation_method: str = "mean_last_layer") -> Dict[str, float]:
        """å°†attentionæƒé‡èšåˆä¸ºtokençº§åˆ«çš„çƒ­åŠ›å›¾"""
        tokens = attention_data["tokens"]
        attentions = attention_data["attentions"]
        
        if aggregation_method == "mean_last_layer":
            # ä½¿ç”¨æœ€åä¸€å±‚çš„å¹³å‡attention
            last_layer_att = attentions[-1]  # (num_heads, seq_len, seq_len)
            # å¯¹æ‰€æœ‰headæ±‚å¹³å‡ï¼Œç„¶åå¯¹æ¯ä¸ªtokençš„incoming attentionæ±‚å’Œ
            token_attention = np.mean(last_layer_att, axis=0).sum(axis=0)
            
        elif aggregation_method == "max_across_layers":
            # è·¨å±‚å–æœ€å¤§attention
            all_layers_att = np.stack([np.mean(att, axis=0).sum(axis=0) for att in attentions])
            token_attention = np.max(all_layers_att, axis=0)
            
        elif aggregation_method == "weighted_layers":
            # ç»™åé¢çš„å±‚æ›´é«˜æƒé‡
            weights = np.linspace(0.1, 1.0, len(attentions))
            weighted_att = []
            for i, att in enumerate(attentions):
                layer_att = np.mean(att, axis=0).sum(axis=0) * weights[i]
                weighted_att.append(layer_att)
            token_attention = np.sum(weighted_att, axis=0)
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        if token_attention.max() > 0:
            token_attention = token_attention / token_attention.max()
        
        # æ„å»ºtokenåˆ°attentionæƒé‡çš„æ˜ å°„
        heatmap = {}
        for i, token in enumerate(tokens):
            if i < len(token_attention):
                heatmap[f"token_{i}_{token}"] = float(token_attention[i])
        
        return heatmap
    
    def simulate_concept_mapping(self, heatmap: Dict[str, float], 
                               concept_nodes: List[str]) -> Dict[str, float]:
        """æ¨¡æ‹Ÿå°†tokençº§attentionæ˜ å°„åˆ°æ¦‚å¿µèŠ‚ç‚¹çš„è¿‡ç¨‹"""
        print("ğŸ—ºï¸ Simulating token-to-concept mapping...")
        
        # ç®€åŒ–çš„æ¦‚å¿µæ˜ å°„ï¼šåŸºäºå…³é”®è¯åŒ¹é…
        concept_attention = {concept: 0.0 for concept in concept_nodes}
        
        for token_key, attention_weight in heatmap.items():
            token = token_key.split("_")[-1].lower().strip("â–")  # å¤„ç†SentencePiece token
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…ç­–ç•¥
            for concept in concept_nodes:
                if token in concept.lower() or concept.lower() in token:
                    concept_attention[concept] += attention_weight
                    
        # å½’ä¸€åŒ–
        max_attention = max(concept_attention.values()) if concept_attention.values() else 1.0
        if max_attention > 0:
            concept_attention = {k: v/max_attention for k, v in concept_attention.items()}
            
        return concept_attention
    
    def visualize_generation_attention_spectrogram(self, generation_data: Dict,
                                                  layer_idx: int = -1, title_suffix: str = ""):
        """å¯è§†åŒ–generationé˜¶æ®µçš„attentioné¢‘è°±å›¾ - ç±»ä¼¼ä½ æè¿°çš„2Då›¾"""
        context_tokens = generation_data["context_tokens"]
        generation_attentions = generation_data["generation_attentions"]
        generated_tokens = generation_data["generated_tokens"]

        if not generation_attentions:
            print("âš ï¸ No generation attention data")
            return

        # æ„å»º2D attentionçŸ©é˜µ: (ç”Ÿæˆæ­¥æ•°, contexté•¿åº¦)
        num_steps = len(generation_attentions)
        context_length = len(context_tokens)

        attention_matrix = np.zeros((num_steps, context_length))

        for step_idx, step_data in enumerate(generation_attentions):
            context_attention = step_data["context_attention"]

            if layer_idx == -1:
                # ä½¿ç”¨æœ€åä¸€å±‚
                layer_attention = context_attention[-1]
            else:
                # ä½¿ç”¨æŒ‡å®šå±‚
                layer_attention = context_attention[layer_idx] if layer_idx < len(context_attention) else context_attention[-1]

            attention_matrix[step_idx, :] = layer_attention

        # åˆ›å»ºé¢‘è°±å›¾
        plt.figure(figsize=(16, 10))

        # ä½¿ç”¨imshowåˆ›å»ºçƒ­åŠ›å›¾
        im = plt.imshow(attention_matrix.T, aspect='auto', cmap='hot', interpolation='nearest')

        # è®¾ç½®æ ‡ç­¾
        plt.title(f'Generation Attention Spectrogram{title_suffix}\n'
                 f'Y-axis: Context Tokens, X-axis: Generated Tokens',
                 fontsize=14, fontweight='bold')
        plt.xlabel('Generation Steps', fontsize=12)
        plt.ylabel('Context Tokens', fontsize=12)

        # è®¾ç½®xè½´æ ‡ç­¾(ç”Ÿæˆçš„tokens)
        x_labels = [f"Step{i+1}\n{token[:8]}" for i, token in enumerate(generated_tokens)]
        plt.xticks(range(len(x_labels)), x_labels, rotation=45, ha='right')

        # è®¾ç½®yè½´æ ‡ç­¾(context tokens) - åªæ˜¾ç¤ºéƒ¨åˆ†ä»¥é¿å…è¿‡äºå¯†é›†
        y_step = max(1, context_length // 20)  # æœ€å¤šæ˜¾ç¤º20ä¸ªæ ‡ç­¾
        y_indices = range(0, context_length, y_step)
        y_labels = [context_tokens[i][:10] for i in y_indices]
        plt.yticks(y_indices, y_labels)

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im)
        cbar.set_label('Attention Weight', rotation=270, labelpad=20)

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        filename = f"generation_attention_spectrogram{title_suffix.replace(' ', '_').lower()}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"ğŸ“Š Spectrogram saved as {filename}")

        return attention_matrix

    def analyze_dynamic_attention_patterns(self, generation_data: Dict) -> Dict:
        """åˆ†æåŠ¨æ€attentionæ¨¡å¼ - è¿™æ˜¯MemoTree LODè°ƒæ•´çš„æ ¸å¿ƒ"""
        context_tokens = generation_data["context_tokens"]
        generation_attentions = generation_data["generation_attentions"]

        print("ğŸ” Analyzing dynamic attention patterns...")

        # åˆ†ææ¯ä¸ªcontext tokenåœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­çš„attentionå˜åŒ–
        context_attention_evolution = {}

        for token_idx, token in enumerate(context_tokens):
            token_attention_over_time = []

            for step_data in generation_attentions:
                # ä½¿ç”¨æœ€åä¸€å±‚çš„attention
                last_layer_attention = step_data["context_attention"][-1]
                token_attention_over_time.append(last_layer_attention[token_idx])

            context_attention_evolution[f"token_{token_idx}_{token}"] = {
                "attention_sequence": token_attention_over_time,
                "max_attention": max(token_attention_over_time),
                "min_attention": min(token_attention_over_time),
                "attention_variance": np.var(token_attention_over_time),
                "final_attention": token_attention_over_time[-1] if token_attention_over_time else 0
            }

        # è¯†åˆ«é«˜åŠ¨æ€æ€§çš„tokens(attentionå˜åŒ–å¤§çš„)
        high_dynamic_tokens = sorted(
            context_attention_evolution.items(),
            key=lambda x: x[1]["attention_variance"],
            reverse=True
        )[:5]

        # è¯†åˆ«æŒç»­é«˜attentionçš„tokens
        high_attention_tokens = sorted(
            context_attention_evolution.items(),
            key=lambda x: x[1]["max_attention"],
            reverse=True
        )[:5]

        print("ğŸ¯ Top 5 High Dynamic Tokens (å˜åŒ–æœ€å¤§):")
        for token_key, stats in high_dynamic_tokens:
            token_name = token_key.split("_")[-1]
            print(f"  {token_name}: variance={stats['attention_variance']:.4f}, "
                  f"max={stats['max_attention']:.4f}")

        print("ğŸ”¥ Top 5 High Attention Tokens (æ³¨æ„åŠ›æœ€é«˜):")
        for token_key, stats in high_attention_tokens:
            token_name = token_key.split("_")[-1]
            print(f"  {token_name}: max={stats['max_attention']:.4f}, "
                  f"final={stats['final_attention']:.4f}")

        return {
            "context_attention_evolution": context_attention_evolution,
            "high_dynamic_tokens": high_dynamic_tokens,
            "high_attention_tokens": high_attention_tokens
        }

def run_generation_experiment():
    """è¿è¡Œæ–°çš„generationé˜¶æ®µattentionåˆ†æå®éªŒ"""
    print("ğŸ§ª Starting MemoTree Generation Attention Experiment")
    print("=" * 60)

    # åˆå§‹åŒ–æå–å™¨
    extractor = AttentionHeatmapExtractor()

    # æµ‹è¯•contextï¼šæ¨¡æ‹ŸMemoTreeçš„è®¤çŸ¥ä¸Šä¸‹æ–‡
    context = """MemoTree is a cognitive context management system for LLM agents. It uses hierarchical LOD (Level of Detail) nodes that can be dynamically expanded or collapsed based on attention patterns. The system integrates with Git for version control and supports semantic relationships between cognitive nodes."""

    print(f"ğŸ“ Context: {context}")
    print()

    # Step 1: æå–generationé˜¶æ®µçš„attention
    print("ğŸš€ Extracting generation attention...")
    generation_data = extractor.extract_generation_attention(
        context=context,
        max_new_tokens=15,
        max_context_length=256
    )

    print(f"âœ… Generated tokens: {generation_data['generated_tokens']}")
    print()

    # Step 2: å¯è§†åŒ–attentioné¢‘è°±å›¾
    print("ğŸ“Š Creating attention spectrogram...")
    attention_matrix = extractor.visualize_generation_attention_spectrogram(
        generation_data,
        layer_idx=-1,  # ä½¿ç”¨æœ€åä¸€å±‚
        title_suffix=" - Last Layer"
    )

    # Step 3: åˆ†æåŠ¨æ€attentionæ¨¡å¼
    print("ğŸ” Analyzing dynamic patterns...")
    pattern_analysis = extractor.analyze_dynamic_attention_patterns(generation_data)

    # Step 4: æ¨¡æ‹ŸLODè°ƒæ•´å†³ç­–
    print("ğŸ¯ Simulating LOD adjustment decisions...")

    # åŸºäºattentionåˆ†æç»“æœæ¨¡æ‹ŸLODè°ƒæ•´
    lod_decisions = []

    for token_key, stats in pattern_analysis["high_attention_tokens"]:
        token_name = token_key.split("_")[-1]
        max_attention = stats["max_attention"]
        final_attention = stats["final_attention"]

        # ç®€å•çš„LODè°ƒæ•´é€»è¾‘
        if max_attention > 0.1 and final_attention > 0.05:
            decision = "EXPAND"  # é«˜attentionä¸”æŒç»­ -> å±•å¼€
        elif max_attention < 0.02:
            decision = "COLLAPSE"  # ä½attention -> æŠ˜å 
        else:
            decision = "MAINTAIN"  # ä¸­ç­‰attention -> ä¿æŒ

        lod_decisions.append({
            "token": token_name,
            "max_attention": max_attention,
            "final_attention": final_attention,
            "lod_decision": decision
        })

    print("ğŸ“‹ LOD Adjustment Decisions:")
    for decision in lod_decisions:
        print(f"  {decision['token']}: {decision['lod_decision']} "
              f"(max={decision['max_attention']:.4f}, final={decision['final_attention']:.4f})")

    # Step 5: ä¿å­˜å®Œæ•´ç»“æœ
    results = {
        "context": context,
        "generation_data": {
            "context_tokens": generation_data["context_tokens"],
            "generated_tokens": generation_data["generated_tokens"],
            "total_steps": generation_data["total_steps"]
        },
        "attention_matrix": attention_matrix.tolist(),
        "pattern_analysis": {
            "high_dynamic_tokens": [(k, v) for k, v in pattern_analysis["high_dynamic_tokens"]],
            "high_attention_tokens": [(k, v) for k, v in pattern_analysis["high_attention_tokens"]]
        },
        "lod_decisions": lod_decisions,
        "model_info": {
            "model_path": extractor.model.config.name_or_path,
            "num_layers": extractor.model.config.num_hidden_layers,
            "num_heads": extractor.model.config.num_attention_heads
        }
    }

    with open("generation_attention_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("ğŸ’¾ Results saved to generation_attention_results.json")
    print("ğŸ‰ Generation experiment completed successfully!")

    return results

if __name__ == "__main__":
    # è¿è¡Œæ–°çš„generationå®éªŒ
    results = run_generation_experiment()
